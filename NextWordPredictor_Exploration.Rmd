---
title: "Next Word Prediction - Exploratory Analysis"
author: "Ash Chakraborty"
date: "March 15, 2016"
output: html_document
---

```{r global, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
library(ggplot2)
library(tm)
library(tm.plugin.dc)
```  

# Overview  

As engagement with mobile devices becomes second nature for us, we're experiencing more effective means of capturing the intended content of communication on these devices, viz. dynamic spell-checkers, swipe-able keyboards, and **next word predictors**. The aim of this analysis is to lay the foundations for a word prediction app. that may be used in a similar vain to that of [Swiftkey's](https://swiftkey.com/en/company) next word prediction feature in their keyboard application.  

Specifically, this report's objectives are the following:  
* Load and clean the input datasets required to train the prediction model,  
* Explore summary statistics and analyze the dataset for other revelations,  
* Layout a blueprint for the prediction algorithm and the prediction application.  

# Data  
 
This exercise uses the files named _LOCALE.blogs.txt_ where _LOCALE_ is each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called [HC Corpora](www.corpora.heliohost.org). From the site: _HC corpora is a collection of corpora for various languages freely available to download. The corpora have been collected from numerous different webpages, with the aim of getting a varied and comprehensive corpus of current use of the respective language._  

I first read in the text files and unpack them into what turns out to be very large Corpora containing the following corpus types:  

```{r input, echo=FALSE}
filePath <- '/Users/ash/Downloads/final/en_US/'
corp <- Corpus(DirSource(filePath))
```  

* Blog Text: A collection of blog posts of total size ```r round(object.size(corp[[1]][1])/1024)``` Kb, and ```r length(corp[[1]][[1]])``` lines.  

* News Text: A collection of news articles of total size ```r round(object.size(corp[[2]][1])/1024)``` Kb, and ```r length(corp[[2]][[1]])``` lines.  

* Twitter Tweets: A collection of Tweets of total size ```r round(object.size(corp[[1]][1])/1024)``` Kb, and ```r length(corp[[3]][[1]])``` lines.  

## Word Tokenizing and Normalization  

Each of the corpus(es?) in the Corpora are stored in three character vectors in the "corp" logical object. In order to get a better understanding of each, it is necessary to first perform the following pre-processing steps:  

### Lowercase and Numbers 

* Blogs: Convert to lowercase to get a sense of word distributions. Acronyms aren't very domain relevant in this case so those needn't be preserved;  
* News: Acronyms might have greater impact here. Preserve uppercase. Also since news sources are usually professionally curated and exhibit a stricter adherence to grammatical rules for caps and acronyms;  
* Tweets: Convert to lowercase  

```{r lower, echo=FALSE}

#corp <- tm_map(corp, tolower)

# #blogs
corp[[1]][[1]] <- tolower(corp[[1]][[1]])
# 
# #news - not forcing lowercase
corp[[2]][[1]] <- tolower(corp[[2]][[1]])
# 
# #tweets
corp[[3]][[1]] <- tolower(corp[[3]][[1]])

#remove all numbers
corp <- tm_map(corp, removeNumbers)

```  


### Punctuation Characters  

Attempt to remove special characters that result in emoticons, that are apostrophes, periods, commas, semicolons, excessive punctuation.  

Exceptions: Preserve the octothorpe "#" character in the Tweets corpora to analyze significance of tweeted hashtag context.

```{r frags, echo=FALSE}
#strip punctuation characters from blogs and news corpora
corp[[1]][[1]] <- removePunctuation(corp[[1]][[1]], preserve_intra_word_dashes = FALSE)

#preserve the "-" in news articles
corp[[2]][[1]] <- removePunctuation(corp[[2]][[1]], preserve_intra_word_dashes = TRUE)

#strip all but octothorpe from twitter dataset
corp[[3]][[1]] <- gsub('[!"$%&\'()*+,-./:;<=>?@\\^_`|~]','',corp[[3]][[1]])

#remove unecessary whitespace
corp <- tm_map(corp, stripWhitespace)

```  

### Document Summaries  

The following table shows the approximate token and type counts for each document type:  

```{r tokentype, echo=FALSE}

docSumm <- data.frame(document.source = 'Blogs', 
                     size_MB = round(object.size(corp[[1]][1])/(2^20))[[1]],
                     lines = length(corp[[1]][[1]]),
                     tokens = sum(unlist(lapply(corp[[1]][[1]], 
                                                function(x){
                                                        length(stri_split_fixed(x," ", 
                                                                                omit_empty = TRUE)[[1]])
                                                        }))))
docSumm <- rbind(docSumm, data.frame(document.source = 'News', 
                 size_MB = round(object.size(corp[[2]][1])/(2^20))[[1]],
                 lines = length(corp[[2]][[1]]),
                 tokens = sum(unlist(lapply(corp[[2]][[1]], 
                                            function(x){
                                                    length(stri_split_fixed(x," ", 
                                                                            omit_empty = TRUE)[[1]])
                                                        })))))
docSumm <- rbind(docSumm, data.frame(document.source = 'Twitter', 
                 size_MB = round(object.size(corp[[3]][1])/(2^20))[[1]],
                 lines = length(corp[[3]][[1]]),
                 tokens = sum(unlist(lapply(corp[[3]][[1]], 
                                            function(x){
                                                    length(stri_split_fixed(x," ", 
                                                                            omit_empty = TRUE)[[1]])
                                                        })))))

knitr::kable(docSumm, row.names=NA,align='c', format.args=list(big.mark=','))

```

### Stop Words  

```{r stopwords, echo=FALSE}
corp <- tm_map(corp, removeWords, stopwords("english"))
```  

### Stemming  

We chop the word *affixes* from their *stems* to improve type grouping.  

```{r stemming, echo=FALSE}
corp <- tm_map(corp, stemDocument)


```  



### Lemmatization  

For the entire Corpora, we want to consider equivalency relations for the following strings: 



```{r}

#treat the pre-processed document as plain text document
corp <- tm_map(corp, PlainTextDocument)
```   

## Term Document Matrices 


```{r dtm, echo=FALSE}

#sampling from the large corpus


sub.corp1 <- Corpus(VectorSource(c(sample(corp[[1]][[1]],
                                           size = length(corp[[1]][[1]])*0.01,
                                           replace = FALSE),
                                    sample(corp[[2]][[1]],
                                           size = length(corp[[2]][[1]])*0.01,
                                           replace = FALSE),
                                    sample(corp[[3]][[1]],
                                           size = length(corp[[3]][[1]])*0.01,
                                           replace = FALSE))))

sub.corp2 <- Corpus(VectorSource(c(sample(corp[[1]][[1]],
                                           size = length(corp[[1]][[1]])*0.01,
                                           replace = FALSE),
                                    sample(corp[[2]][[1]],
                                           size = length(corp[[2]][[1]])*0.01,
                                           replace = FALSE),
                                    sample(corp[[3]][[1]],
                                           size = length(corp[[3]][[1]])*0.01,
                                           replace = FALSE))))

system.time(tdm1 <- TermDocumentMatrix(sub.corp1, control = list(minWordLength=2)))

```





